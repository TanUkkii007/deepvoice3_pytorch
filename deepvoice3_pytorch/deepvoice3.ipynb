{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import Conv1d, ConvTranspose1d, Embedding, Linear, GradMultiply\n",
    "from modules import get_mask_from_lengths, SinusoidalEncoding, Conv1dGLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function bmm in module torch._C:\n",
      "\n",
      "bmm(...)\n",
      "    bmm(batch1, batch2, out=None) -> Tensor\n",
      "    \n",
      "    Performs a batch matrix-matrix product of matrices stored in :attr:`batch1`\n",
      "    and :attr:`batch2`.\n",
      "    \n",
      "    :attr:`batch1` and :attr:`batch2` must be 3D Tensors each containing\n",
      "    the same number of matrices.\n",
      "    \n",
      "    If :attr:`batch1` is a `b x n x m` Tensor, :attr:`batch2` is a `b x m x p`\n",
      "    Tensor, :attr:`out` will be a `b x n x p` Tensor.\n",
      "    \n",
      "    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
      "              For broadcasting matrix products, see :func:`torch.matmul`.\n",
      "    \n",
      "    Args:\n",
      "        batch1 (Tensor): First batch of matrices to be multiplied\n",
      "        batch2 (Tensor): Second batch of matrices to be multiplied\n",
      "        out (Tensor, optional): Output tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> batch1 = torch.randn(10, 3, 4)\n",
      "        >>> batch2 = torch.randn(10, 4, 5)\n",
      "        >>> res = torch.bmm(batch1, batch2)\n",
      "        >>> res.size()\n",
      "        torch.Size([10, 3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.bmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot-product attention\n",
    "\n",
    "$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})V$\n",
    "\n",
    "source length: $N$, target length: $M$\n",
    "\n",
    "$\\mathrm{softmax_{over-source}}\\left\\{\\left(\\begin{array}{c}\n",
    "\\mathbf{q}_1\\\\\n",
    "\\hline\\\\\n",
    "\\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\mathbf{q}_M\\end{array}\\right)\n",
    "\\left(\\begin{array}{c|c|c}\n",
    "\\mathbf{k}_1 & \\dots & \\mathbf{k}_N\n",
    "\\end{array}\\right)\n",
    "\\right\\}\n",
    "\\left(\\begin{array}{c}\n",
    "\\mathbf{v}_1\\\\\n",
    "\\hline\\\\\n",
    "\\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\mathbf{v}_N\n",
    "\\end{array}\\right)$\n",
    "\n",
    "$ = \\left(\\begin{array}{ccc}\n",
    "\\mathrm{softmax_{row-wise}}\\left(\\mathbf{q}_1 \\cdot \\mathbf{k}_1\\right) & \\dots & \\mathrm{softmax_{row-wise}}\\left(\\mathbf{q}_1 \\cdot \\mathbf{k}_N\\right)\\\\\n",
    "\\hline\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\mathrm{softmax_{row-wise}}\\left(\\mathbf{q}_M \\cdot \\mathbf{k}_1\\right) & \\dots & \\mathrm{softmax_{row-wise}}\\left(\\mathbf{q}_M \\cdot \\mathbf{k}_N\\right)\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{c}\n",
    "\\mathbf{v}_1\\\\\n",
    "\\hline\\\\\n",
    "\\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\mathbf{v}_N\n",
    "\\end{array}\\right)$\n",
    "\n",
    "$ = \\left(\\begin{array}{ccc}\n",
    "\\frac{\\exp(\\mathbf{q}_1 \\cdot \\mathbf{k}_1)}{\\sum_n \\exp(\\mathbf{q}_1 \\cdot \\mathbf{k}_n)} & \\dots & \\frac{\\exp(\\mathbf{q}_1 \\cdot \\mathbf{k}_N)}{\\sum_n \\exp(\\mathbf{q}_1 \\cdot \\mathbf{k}_n)}\\\\\n",
    "\\hline\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\frac{\\exp(\\mathbf{q}_M \\cdot \\mathbf{k}_1)}{\\sum_n \\exp(\\mathbf{q}_M \\cdot \\mathbf{k}_n)} & \\dots & \\frac{\\exp(\\mathbf{q}_M \\cdot \\mathbf{k}_N)}{\\sum_n \\exp(\\mathbf{q}_M \\cdot \\mathbf{k}_n)}\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{c}\n",
    "\\mathbf{v}_1\\\\\n",
    "\\hline\\\\\n",
    "\\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\mathbf{v}_N\n",
    "\\end{array}\\right)$\n",
    "\n",
    "$ = \\left(\\begin{array}{c}\n",
    "\\mathbf{a}_1\\\\\n",
    "\\hline\\\\\n",
    "\\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\mathbf{a}_M\n",
    "\\end{array}\\right)\\mathbf{V}$\n",
    "\n",
    "$ = \\left(\\begin{array}{c}\n",
    "\\mathbf{a}_1\\mathbf{V}\\\\\n",
    "\\hline\\\\\n",
    "\\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\mathbf{a}_M\\mathbf{V}\n",
    "\\end{array}\\right)$\n",
    "\n",
    "$ = \\left(\\begin{array}{c}\n",
    "\\sum_{n=1}^N a_{1n}\\mathbf{v}_n\\\\\n",
    "\\hline\\\\\n",
    "\\vdots\\\\\n",
    "\\hline\\\\\n",
    "\\sum_{n=1}^N a_{Mn}\\mathbf{v}_n\n",
    "\\end{array}\\right)$\n",
    "\n",
    "$\\mathbf{a}_m = \\left(\n",
    "\\frac{\\exp(\\mathbf{q}_m \\cdot \\mathbf{k}_1)}{\\sum_n \\exp(\\mathbf{q}_m \\cdot \\mathbf{k}_n)} \n",
    ", \\dots, \n",
    "\\frac{\\exp(\\mathbf{q}_m \\cdot \\mathbf{k}_N)}{\\sum_n \\exp(\\mathbf{q}_m \\cdot \\mathbf{k}_n)}\n",
    "\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, conv_channels, embed_dim, dropout=0.1, window_ahead=3, window_backward=1, key_projection=True, value_projection=True):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.query_projection = Linear(conv_channels, embed_dim)\n",
    "        if key_projection:\n",
    "            self.key_projection = Linear(embed_dim, embed_dim)\n",
    "            # According to the DeepVoice3 paper, intiailize weights to same values\n",
    "            # TODO: Does this really work well? not sure..\n",
    "            # > We initialize the fully-connected layer weights used to compute\n",
    "            #  hidden attention vectors to the same values for the query projection\n",
    "            #  and the key projection.\n",
    "            if conv_channels == embed_dim:\n",
    "                self.key_projection.weight.data = self.query_projection.weight.data.clone()\n",
    "        else:\n",
    "            self.key_projection = None\n",
    "        if value_projection:\n",
    "            self.value_projection = Linear(embed_dim, embed_dim)\n",
    "        else:\n",
    "            self.value_projection = None\n",
    "        \n",
    "        self.out_projection = Linear(embed_dim, conv_channels)\n",
    "        self.dropout = dropout\n",
    "        self.wiindow_ahead = window_ahead\n",
    "        self.window_backward = window_backward\n",
    "        \n",
    "    def forward(self, query, encoder_out, mask=None, last_attended=None):\n",
    "        # query: # (B, tgt_len, conv_channels)\n",
    "        # keys: (B, embed_dim, source_length)\n",
    "        # values: (B, source_length, embed_dim)\n",
    "        keys, values = encoder_out\n",
    "        residual = query\n",
    "        if self.value_projection is not None:\n",
    "            # FC(values)\n",
    "            # values: (B, source_length, embed_dim)\n",
    "            values = self.value_projection(values)\n",
    "        # TODO: yes, this is inefficient\n",
    "        if self.key_projection is not None:\n",
    "            # FC(keys)\n",
    "            # keys: (B, source_length, embed_dim)\n",
    "            keys = self.key_projection(keys.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        # attention\n",
    "        # FC(query)\n",
    "        # x: (B, tgt_len, embed_dim)\n",
    "        x = self.query_projection(query)\n",
    "        # dot(query, keys)\n",
    "        # keys must be a matrix consists of column vectors\n",
    "        # (B, tgt_len, src_len)\n",
    "        x = torch.bmm(x, keys) # batch matrix-matrix product\n",
    "        \n",
    "        mask_value = -float(\"inf\")\n",
    "        if mask is not None:\n",
    "            # inference mask\n",
    "            mask = mask.view(query.size(0), 1, -1)\n",
    "            x.data.masked_fill_(mask, mask_value)\n",
    "        \n",
    "        if last_attended is not None:\n",
    "            backward = last_attended - self.window_backward\n",
    "            if backward > 0:\n",
    "                x[:, :, :backward] = mask_value\n",
    "            ahead = last_attend + self.window_ahead\n",
    "            if ahed < x.size(-1):\n",
    "                x[:, :, ahead:] = mask_value\n",
    "                \n",
    "        # softmax over last dim\n",
    "        # (B, tgt_len, src_len)\n",
    "        sz = x.size()\n",
    "        x = F.softmax(x.view(sz[0] * sz[1], sz[2]), dim=1)\n",
    "        x = x.view(sz)\n",
    "        attn_scores = x\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # dot(softmax(dot(query, keys)), values)\n",
    "        x = torch.bmm(x, values)\n",
    "        \n",
    "        # scale attention output\n",
    "        s = values.size(1)\n",
    "        x = x * (s * math.sqrt(1.0 / s))\n",
    "        \n",
    "        # project back\n",
    "        x = self.out_projection(x)\n",
    "        x = (x + residual) * math.sqrt(0.5)\n",
    "        return x, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       "  -11.6128   0.0791 -14.0917  -3.4851   8.3419\n",
       "   -1.5029   0.6000  -1.9552  -0.0414   2.0806\n",
       "  -11.6128   0.0791 -14.0917  -3.4851   8.3419\n",
       "  -11.6128   0.0791 -14.0917  -3.4851   8.3419\n",
       " [torch.FloatTensor of size 1x4x5], Variable containing:\n",
       " (0 ,.,.) = \n",
       "   0.0828  0.2373  0.6799\n",
       "   0.0828  0.2373  0.6799\n",
       "   0.0828  0.2373  0.6799\n",
       "   0.0828  0.2373  0.6799\n",
       " [torch.FloatTensor of size 1x4x3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_length = 4\n",
    "source_length = 3\n",
    "embed_dim = 2\n",
    "conv_channels = 5\n",
    "\n",
    "attention_layer = AttentionLayer(conv_channels=conv_channels, embed_dim=embed_dim)\n",
    "\n",
    "query = Variable(torch.ones(1, query_length, conv_channels))\n",
    "encoder_out = Variable(torch.from_numpy(np.array(range(source_length * embed_dim), dtype=np.float32).reshape(1, source_length, embed_dim)))\n",
    "\n",
    "attention_layer(query=query, encoder_out=(encoder_out.transpose(1,2), encoder_out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
